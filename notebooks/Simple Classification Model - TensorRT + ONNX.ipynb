{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684548b2",
   "metadata": {},
   "source": [
    "## ONNX + TenorRT\n",
    "\n",
    "There are several pathways one can choose from, in order to take a model from development to production. A clear winner amongst all existing methods is TensorRT, NVIDIA's flagship model optimization framework and inference engine generator. In simple words, TensorRT takes a Torch/TensorFlow model and converts it into an \"engine\" such that it makes most of the existing hardware resource. We will first look at a simple example of converting a classification model into TensorRT engine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c983e7-a4f2-4393-8a1d-030dbdf311e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb001fd-ce6f-4946-bb2a-5234fe3c2640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    # transformations for the input data\n",
    "    transforms = Compose([\n",
    "        ToTensor(),\n",
    "        Resize(224),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # read input image\n",
    "    input_img = cv2.imread(img_path)\n",
    "    # do transformations\n",
    "    input_data = transforms(input_img)\n",
    "    batch_data = torch.unsqueeze(input_data, 0)\n",
    "    return batch_data\n",
    "\n",
    "def postprocess(output_data):\n",
    "    # get class names\n",
    "    with open(\"../data/imagenet_classes.txt\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    # calculate human-readable value by softmax\n",
    "    confidences = torch.nn.functional.softmax(output_data, dim=1)[0] * 100\n",
    "    # find top predicted classes\n",
    "    _, indices = torch.sort(output_data, descending=True)\n",
    "    i = 0\n",
    "    # print the top classes predicted by the model\n",
    "    while confidences[indices[0][i]] > 0.5:\n",
    "        class_idx = indices[0][i]\n",
    "        print(\n",
    "            \"class:\",\n",
    "            classes[class_idx],\n",
    "            \", confidence:\",\n",
    "            confidences[class_idx].item(),\n",
    "            \"%, index:\",\n",
    "            class_idx.item(),\n",
    "        )\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311dde19",
   "metadata": {},
   "source": [
    "###  Step 1 : Just Torch based inference\n",
    "Here, we use a pretrained Resnet50 model to classify an input image. The inference in done purely in Pytorch. The model's prediction is passed through a post processing engine to get final predction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3358a98e-68c2-4bd5-bcec-6e3fd0e04c43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeeshan-motorai/anaconda3/envs/tensorrt/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zeeshan-motorai/anaconda3/envs/tensorrt/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: cup , confidence: 94.97859191894531 %, index: 968\n",
      "class: espresso , confidence: 3.951244831085205 %, index: 967\n",
      "class: coffee mug , confidence: 0.6196929216384888 %, index: 504\n"
     ]
    }
   ],
   "source": [
    "input = preprocess_image(\"../data/turkish_coffee.jpg\").cuda()\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "model.cuda()\n",
    "output = model(input)\n",
    "\n",
    "postprocess(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68601ac",
   "metadata": {},
   "source": [
    "### Step 2 : Convert Model to ONNX\n",
    "Here, we first convert the given model to ONNX representation which will be later converted to TensorRT engine. There are several ways tto convert a model to TensorRT, but the most common method is using ONNX representation. We pass dummy input of the same shape as expected inputs, the model instance to the export function to convert a given torch network to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387ab26d-4642-494a-bd3a-0f2349baa471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ONNX_FILE_PATH = '../onnx_files/resnet50.onnx'\n",
    "torch.onnx.export(model, input, ONNX_FILE_PATH, input_names=['input'],\n",
    "                  output_names=['output'], export_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd3e63",
   "metadata": {},
   "source": [
    "### Step 3: Convert ONNX model to TensorRT engine\n",
    "Now, we finally convert this generated ONNX model to TensorRT engine. This process involves several steps and we will look at them below :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837f05d",
   "metadata": {},
   "source": [
    "The generated ONNX file is saved in the [onnx_files](../onnx_files) folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16873f1e-5691-4344-aac8-0bcdf3c316e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import tensorrt as trt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a2f6c",
   "metadata": {},
   "source": [
    "#### 1. Create Builder\n",
    "To create a builder, you must first create a logger. Then use the logger to create the builder.  Builder allows the creation of an optimized engine from a network definition. It allows the application to specify the maximum batch and workspace size, the minimum acceptable level of precision, timing iteration counts for autotuning, and an interface for quantizing networks to run in 8-bit precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3eb1f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/14/2023-17:00:50] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[04/14/2023-17:00:50] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2563, GPU 2148 (MiB)\n"
     ]
    }
   ],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "builder = trt.Builder(TRT_LOGGER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a0636",
   "metadata": {},
   "source": [
    "#### 2. Create Network\n",
    "After the builder has been created, the first step in optimizing a model is to create a network definition. The EXPLICIT_BATCH flag is required in order to import models using the ONNX parser.  Network Definition provides methods for the application to specify the definition of a network. Input and output tensors can be specified, layers can be added, and there is an interface for configuring each supported layer type.\n",
    "Layers like convolutional and recurrent layers, and a Plugin layer type allows the application to implement functionality not natively supported by TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e99cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ba1d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorrt.tensorrt.INetworkDefinition at 0x7fee02fb0fb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac991a",
   "metadata": {},
   "source": [
    "#### 3. Import model using ONNX Parser\n",
    "Now, the network definition must be populated from the ONNX representation. You can create an ONNX parser to populate the network as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c858b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04/14/2023-17:00:53] [TRT] [I] ----------------------------------------------------------------\n",
      "[04/14/2023-17:00:53] [TRT] [I] Input filename:   ../onnx_files/resnet50.onnx\n",
      "[04/14/2023-17:00:53] [TRT] [I] ONNX IR version:  0.0.7\n",
      "[04/14/2023-17:00:53] [TRT] [I] Opset version:    13\n",
      "[04/14/2023-17:00:53] [TRT] [I] Producer name:    pytorch\n",
      "[04/14/2023-17:00:53] [TRT] [I] Producer version: 1.12.1\n",
      "[04/14/2023-17:00:53] [TRT] [I] Domain:           \n",
      "[04/14/2023-17:00:53] [TRT] [I] Model version:    0\n",
      "[04/14/2023-17:00:53] [TRT] [I] Doc string:       \n",
      "[04/14/2023-17:00:53] [TRT] [I] ----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "success = parser.parse_from_file(ONNX_FILE_PATH)\n",
    "for idx in range(parser.num_errors):\n",
    "    print(parser.get_error(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6343b",
   "metadata": {},
   "source": [
    "#### 4. Building an engine\n",
    "The next step is to create a build configuration specifying how TensorRT should optimize the model. This interface has many properties that you can set in order to control how TensorRT optimizes the network. \n",
    " Allows the application to execute inference. \n",
    "   - It supports synchronous and asynchronous execution, profiling, and enumeration and querying of the bindings for the engine inputs and outputs. \n",
    "   - A single-engine can have multiple execution contexts, allowing a single set of trained parameters to be used for the simultaneous execution of multiple batches.\n",
    "\n",
    "One important property is the maximum workspace size. Layer implementations often require a temporary workspace, and this parameter limits the maximum size that any layer in the network can use. If insufficient workspace is provided, it is possible that TensorRT will not be able to find an implementation for a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce55bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = builder.create_builder_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4109c99a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorrt.tensorrt.IBuilderConfig' object has no attribute 'set_memory_pool_limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_memory_pool_limit\u001b[49m(trt\u001b[38;5;241m.\u001b[39mMemoryPoolType\u001b[38;5;241m.\u001b[39mWORKSPACE, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m20\u001b[39m) \u001b[38;5;66;03m# 1 MiB\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorrt.tensorrt.IBuilderConfig' object has no attribute 'set_memory_pool_limit'"
     ]
    }
   ],
   "source": [
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 20) # 1 MiB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2dc8f8",
   "metadata": {},
   "source": [
    "After the configuration has been specified, the engine can be built and serialized with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_engine = builder.build_serialized_network(network, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4243997",
   "metadata": {},
   "source": [
    "It may be useful to save the engine to a file for future use. You can do that like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../trt_engines/sample.engine\", \"wb\") as f:\n",
    "    f.write(serialized_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56c419",
   "metadata": {},
   "source": [
    "#### 5. Deserialize an Engine\n",
    "To perform inference, deserialize the engine using the Runtime interface. Like the builder, the runtime requires an instance of the logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762874ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = trt.Runtime(TRT_LOGGER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b470f8e",
   "metadata": {},
   "source": [
    "First load the engine from a file. Then deserialize the engine from a memory buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../trt_engines/sample.engine\", \"rb\") as f:\n",
    "    serialized_engine = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42324492",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = runtime.deserialize_cuda_engine(serialized_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c44d1",
   "metadata": {},
   "source": [
    "#### 6. Performing Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd66913",
   "metadata": {},
   "source": [
    "The engine holds the optimized model, but to perform inference requires additional state for intermediate activations. An engine can have multiple execution contexts, allowing one set of weights to be used for multiple overlapping inference tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a99caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = engine.create_execution_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465dbb6",
   "metadata": {},
   "source": [
    "Allocate some host and device buffers for inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) to hold host inputs/outputs.\n",
    "h_input = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=np.float32)\n",
    "h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=np.float32)\n",
    "# Allocate device memory for inputs and outputs.\n",
    "d_input = cuda.mem_alloc(h_input.nbytes)\n",
    "d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "# Create a stream in which to copy inputs/outputs and run inference.\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d339b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_input = np.array(preprocess_image(\"../data/turkish_coffee.jpg\").numpy(), dtype=np.float32, order='C')\n",
    "# Transfer input data to the GPU.\n",
    "cuda.memcpy_htod_async(d_input, host_input, stream)\n",
    "# Run inference.\n",
    "context. execute_async_v2(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)\n",
    "# Transfer predictions back from the GPU.\n",
    "cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "# Synchronize the stream\n",
    "stream.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf881178",
   "metadata": {},
   "source": [
    "Create some space to store intermediate activation values. Since the engine holds the network definition and trained parameters, additional space is necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4719b676-34a1-4107-95c3-338e619c99ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_data = torch.Tensor(h_output).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa64093",
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4ef79",
   "metadata": {},
   "source": [
    "Finally we are able to recreate the same results that we obtained using pure pytorch model. In conclusion, we first converted a given model to it's ONNX representation, then used this ONNX representation to generate a TensorRT engine. Then, use this saved engine for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd2e805bf45cfce0ee59caeff624ebf2dd294551239d1fbf11f6751485705e2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
